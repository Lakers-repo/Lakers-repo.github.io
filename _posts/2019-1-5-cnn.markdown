---
layout: post
title:  "卷积神经网络概述"
date:   2019-1-5
categories: MachineLearning
keywords: CNN DeepLearning
mathjax: true
author: wzx
---

卷积神经网络 (Convolutional Neural Network)，由若干卷积层，池化层和全连接层组成，与其他深度学习结构相比，卷积神经网络在图像和语音识别方面能够给出更好的结果
- 卷积层 Convolution(CONV)
- 池化层 Pooling(POOL)
- 全连接层 Fully connected(FC)




## 特有结构
### 卷积层
个人理解，这层的卷积核就类似全连接神经网络中的节点。之前，我们将每个节点中的权重值与输入值相乘，而在卷积层中则是将数据与卷积核进行卷积运算

#### 卷积
通俗理解就是将卷积核依次在输入矩阵中滑动，并且对应元素相乘并求和
![]({{ site.url }}/assets/img/2019-1-5-1.gif)  
我们可以通过 **filter size(f)** 来指定卷积核的大小  
我们可以通过 **步长(step)** 来指定卷积核每次滑动的距离  
输入数据每经过一层卷积层时，数据量都会缩小，所以我们可以通过指定 **padding** 减少卷积运算后的数据量的缩小，另一方面，也可以防止输入数据的边缘采样过少。一般来说，我们采用以下两种 *pad* 方式。**Valid Convolution** 为 padding = 0 的情况，**Same Convolution** 为卷积后的数据量与输入数据量相同  

  ![]({{ site.url }}/assets/img/2019-1-5-2.png)

#### 多通道的卷积
如果是灰度图，那么直接进行卷积运算即可，但是我们在运用 *CNN* 的时，大多数情况是 RGB 带有三个通道的彩色图像，这时，我们将卷积核复制三份堆叠，依次对每个颜色通道进行卷积并求和，便完成了多通道的卷积
![]({{ site.url }}/assets/img/2019-1-5-3.png)

#### 卷积的输出大小
$$ \lceil \frac{n+2p-f}{s}+1 \rceil $$
- n ：输入数据size
- p ：padding
- f ：filter size
- s ：step
- 向下取整

### 池化层
池化层的作用主要是通过降采样的方式，压缩数据量，减少参数
#### Max Pooling
最大池化运算与卷积运算类似，通过指定 **filter size(f)** **step(s)** 这些超参数来调整池化层。运算结果就是求范围内的最大值，比较常用
![]({{ site.url }}/assets/img/2019-1-5-4.png)
#### Average Pooling
求范围内的平均值，其他与最大池化类似

## 如何连接
### 单个卷积层的运算
一个卷积层中含有的卷积核，就像 全连接的**DNN**中每层的节点一样。输入数据分别与卷积核进行卷积运算，并加上偏置量，经过激活函数后得到的便是这个卷积核的输出值。所以该卷积层拥有多少卷积核便会输出多少通道的数据
![]({{ site.url }}/assets/img/2019-1-5-5.png)

### 卷积神经网络的连接
卷积神经网络具有这样的特点，随着层数的增加，数据量的尺寸会减小，但它的通道数量会增加。其结构是这样的，一个或多个卷积层后连接一个池化层，经过若干这样的连接后，连接全连接层，最后经softmax层或其他获取输出值。特别地，由于池化层只含有一些超参数，并没有权重参数，所以我们一般不把池化层算作单独的一层
![]({{ site.url }}/assets/img/2019-1-5-6.png)

## 优势
### 参数共享
一个特征识别器，即卷积核，对图像的某部分有效，那同样对其他部分也有效。对图像做卷积运算，就相当于共享卷积核中的参数，从而达到减少参数的目的
### 疏松连接
每个输出值只与那一部分输入值有关，而普通神经网络的输出值是由所有输入值共同决定的，这就大大减少了参数

## Demo
[Convolutional Neural Network](https://github.com/wzx140/CNN_demo)
